{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import keras.backend as K\n",
    "from keras.engine.topology import Layer\n",
    "from keras.layers import Conv2D, Reshape, Lambda\n",
    "from keras.activations import softmax\n",
    "\n",
    "\n",
    "def margin_loss(y_true, y_pred, \n",
    "    m_plus=0.9, m_minus=0.1, down_weighting=0.5):\n",
    "    \n",
    "    L = y_true*K.square(K.maximum(0.0, m_plus-y_pred)) + down_weighting*(1-y_true)*K.square(K.maximum(0.0, y_pred-m_minus))\n",
    "\n",
    "    return K.mean(K.sum(L, 1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pylab as plt\n",
    "import cv2\n",
    "import os\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import mlxtend.plotting\n",
    "from tqdm import tqdm\n",
    "from IPython.display import clear_output\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from tensorflow.keras.models import  Model\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from tensorflow.keras.optimizers import Adam,Adamax\n",
    "from tensorflow.keras.layers import Conv2D, SeparableConv2D, Dense, Dropout, concatenate, MaxPooling2D, BatchNormalization, GlobalAveragePooling2D, Input\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import tensorflow\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.keras import activations\n",
    "from tensorflow.keras import utils\n",
    "\n",
    "from tensorflow.keras.layers import *\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE=50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=np.load('E:/thorax_data.npy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=train[:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_NMF_G = np.array([i[0] for i in train]).reshape(-1,IMG_SIZE,IMG_SIZE,1)\n",
    "X_NMF_A = np.array([i[0] for i in train]).reshape(-1,IMG_SIZE,IMG_SIZE,1)\n",
    "X = np.array([i[1] for i in train]).reshape(-1,IMG_SIZE,IMG_SIZE,1)\n",
    "YD = np.array([i[2] for i in train])\n",
    "YG = np.array([i[3] for i in train])\n",
    "YA = np.array([i[4] for i in train])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "XG, XGt, YG1, YGt1 = train_test_split(X_NMF_G, YG, test_size=0.1, random_state=42)\n",
    "XA, XAt, YA1, YAt1 = train_test_split(X_NMF_A, YA, test_size=0.1, random_state=42)\n",
    "X, Xt, Y1, Yt1 = train_test_split(X, YD, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=X/255\n",
    "Xt=Xt/255\n",
    "XG=XG/255\n",
    "XA=XA/255\n",
    "XGt=XGt/255\n",
    "XAt=XAt/255\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "def weighted_categorical_crossentropy(weights):\n",
    "    \"\"\"\n",
    "    A weighted version of keras.objectives.categorical_crossentropy\n",
    "    \n",
    "    Variables:\n",
    "        weights: numpy array of shape (C,) where C is the number of classes\n",
    "    \n",
    "    Usage:\n",
    "        weights = np.array([0.5,2,10]) # Class one at 0.5, class 2 twice the normal weights, class 3 10x.\n",
    "        loss = weighted_categorical_crossentropy(weights)\n",
    "        model.compile(loss=loss,optimizer='adam')\n",
    "    \"\"\"\n",
    "    \n",
    "    weights = K.variable(weights)\n",
    "        \n",
    "    def loss(y_true, y_pred):\n",
    "        # scale predictions so that the class probas of each sample sum to 1\n",
    "        y_pred /= K.sum(y_pred, axis=-1, keepdims=True)\n",
    "        # clip to prevent NaN's and Inf's\n",
    "        y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n",
    "        # calc\n",
    "        loss = y_true * K.log(y_pred) * weights\n",
    "        loss = -K.sum(loss, -1)\n",
    "        return loss\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def dice_coef(y_true, y_pred, smooth=1):\n",
    "    \"\"\"\n",
    "    Dice = (2*|X & Y|)/ (|X|+ |Y|)\n",
    "         =  2*sum(|A*B|)/(sum(A^2)+sum(B^2))\n",
    "    ref: https://arxiv.org/pdf/1606.04797v1.pdf\n",
    "    \"\"\"\n",
    "    intersection = K.sum(K.abs(y_true * y_pred), axis=-1)\n",
    "    return (2. * intersection + smooth) / (K.sum(K.square(y_true),-1) + K.sum(K.square(y_pred),-1)+ smooth)\n",
    "\n",
    "def dice_coef_loss(y_true, y_pred):\n",
    "    return (1-dice_coef(y_true, y_pred))\n",
    "def dice_coef_loss2(y_true, y_pred):\n",
    "    \n",
    "    return 0.8*(1-dice_coef(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coef(y_true, y_pred, smooth=1):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "\n",
    "\n",
    "def dice_coef_loss(y_true, y_pred):\n",
    "    return 1 - dice_coef(y_true, y_pred)\n",
    "\n",
    "\n",
    "def tversky(y_true, y_pred, smooth=1, alpha=0.7):\n",
    "    y_true_pos = K.flatten(y_true)\n",
    "    y_pred_pos = K.flatten(y_pred)\n",
    "    true_pos = K.sum(y_true_pos * y_pred_pos)\n",
    "    false_neg = K.sum(y_true_pos * (1 - y_pred_pos))\n",
    "    false_pos = K.sum((1 - y_true_pos) * y_pred_pos)\n",
    "    return (true_pos + smooth) / (true_pos + alpha * false_neg + (1 - alpha) * false_pos + smooth)\n",
    "\n",
    "\n",
    "def tversky_loss(y_true, y_pred):\n",
    "    return 1 - tversky(y_true, y_pred)\n",
    "\n",
    "\n",
    "def focal_tversky_loss(y_true, y_pred, gamma=0.75):\n",
    "    tv = tversky(y_true, y_pred)\n",
    "    return K.pow((1 - tv), gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "gamma=2.\n",
    "alpha=4.\n",
    "\n",
    "gamma = float(gamma)\n",
    "alpha = float(alpha)\n",
    "\n",
    "def focal_loss_fixed(y_true, y_pred):\n",
    "    \"\"\"Focal loss for multi-classification\n",
    "    FL(p_t)=-alpha(1-p_t)^{gamma}ln(p_t)\n",
    "    Notice: y_pred is probability after softmax\n",
    "    gradient is d(Fl)/d(p_t) not d(Fl)/d(x) as described in paper\n",
    "    d(Fl)/d(p_t) * [p_t(1-p_t)] = d(Fl)/d(x)\n",
    "    Focal Loss for Dense Object Detection\n",
    "    https://arxiv.org/abs/1708.02002\n",
    "\n",
    "    Arguments:\n",
    "    y_true {tensor} -- ground truth labels, shape of [batch_size, num_cls]\n",
    "            y_pred {tensor} -- model's output, shape of [batch_size, num_cls]\n",
    "\n",
    "    Keyword Arguments:\n",
    "            gamma {float} -- (default: {2.0})\n",
    "            alpha {float} -- (default: {4.0})\n",
    "\n",
    "        Returns:\n",
    "            [tensor] -- loss.\n",
    "    \"\"\"\n",
    "    epsilon = 1.e-9\n",
    "    y_true = tf.convert_to_tensor(y_true, tf.float32)\n",
    "    y_pred = tf.convert_to_tensor(y_pred, tf.float32)\n",
    "\n",
    "    model_out = tf.add(y_pred, epsilon)\n",
    "    ce = tf.multiply(y_true, -tf.log(model_out))\n",
    "    weight = tf.multiply(y_true, tf.pow(tf.subtract(1., model_out), gamma))\n",
    "    fl = tf.multiply(alpha, tf.multiply(weight, ce))\n",
    "    reduced_fl = tf.reduce_max(fl, axis=1)\n",
    "    return tf.reduce_mean(reduced_fl)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def jaccard_distance_loss(y_true, y_pred, smooth=100):\n",
    "    \"\"\"\n",
    "    Jaccard = (|X & Y|)/ (|X|+ |Y| - |X & Y|)\n",
    "            = sum(|A*B|)/(sum(|A|)+sum(|B|)-sum(|A*B|))\n",
    "    \n",
    "    The jaccard distance loss is usefull for unbalanced datasets. This has been\n",
    "    shifted so it converges on 0 and is smoothed to avoid exploding or disapearing\n",
    "    gradient.\n",
    "    \n",
    "    Ref: https://en.wikipedia.org/wiki/Jaccard_index\n",
    "    \n",
    "    @url: https://gist.github.com/wassname/f1452b748efcbeb4cb9b1d059dce6f96\n",
    "    @author: wassname\n",
    "    \"\"\"\n",
    "    intersection = K.sum(K.abs(y_true * y_pred), axis=-1)\n",
    "    sum_ = K.sum(K.abs(y_true) + K.abs(y_pred), axis=-1)\n",
    "    jac = (intersection + smooth) / (sum_ - intersection + smooth)\n",
    "    return (1 - jac) * smooth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlotLearning(Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.i = 0\n",
    "        self.x = []\n",
    "        self.losses = []\n",
    "        self.val_losses = []\n",
    "        self.acc = []\n",
    "        self.val_acc = []\n",
    "        self.fig = plt.figure()\n",
    "\n",
    "        self.logs = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        self.logs.append(logs)\n",
    "        self.x.append(self.i)\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        self.val_losses.append(logs.get('val_loss'))\n",
    "        self.acc.append(logs.get('acc'))\n",
    "        self.val_acc.append(logs.get('val_acc'))\n",
    "        self.i += 1\n",
    "        f, (ax1, ax2) = plt.subplots(1, 2, sharex=True)\n",
    "\n",
    "        clear_output(wait=True)\n",
    "\n",
    "        ax1.set_yscale('Log')\n",
    "        ax1.plot(self.x, self.losses, label=\"loss\")\n",
    "        ax1.plot(self.x, self.val_losses, label=\"val_loss\")\n",
    "        ax1.legend()\n",
    "\n",
    "        ax2.plot(self.x, self.acc, label=\"acc\")\n",
    "        ax2.plot(self.x, self.val_acc, label=\"val_acc\")\n",
    "        ax2.legend()\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "from keras.optimizers import rmsprop, adamax\n",
    "plot = PlotLearning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scheduler(epoch, lr):\n",
    "    if epoch < 10:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr * tensorflow.math.exp(-0.1)\n",
    "callback1 = tensorflow.keras.callbacks.LearningRateScheduler(scheduler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.initializers import glorot_uniform\n",
    "initializer = glorot_uniform()\n",
    "input_layer = Input((50,50, 1))\n",
    "\n",
    "###########################################################\n",
    "#Convolutional Stream1\n",
    "###########################################################\n",
    "\n",
    "Conv11 = Conv2D(32, (3,3), activation='relu')(input_layer)#CB11\n",
    "Conv12 = Conv2D(32, (3,3), activation='relu',padding='same')(Conv11)\n",
    "Conv12 = BatchNormalization()(Conv12)\n",
    "Conv12 = Dropout(0.5)(Conv12)\n",
    "CMaxpool11 = MaxPooling2D((3,3))(Conv12)\n",
    "\n",
    "Conv13 = Conv2D(64, (3,3), activation='relu',padding='same')(CMaxpool11)#CB12\n",
    "Conv14 = Conv2D(64, (3,3), activation='relu',padding='same')(Conv13)\n",
    "Conv14 = BatchNormalization()(Conv14)\n",
    "Conv14 = Dropout(0.5)(Conv14)\n",
    "CMaxpool12 = MaxPooling2D((3,3))(Conv14)\n",
    "\n",
    "Conv15 = Conv2D(128, (3,3), activation='relu',padding='same')(CMaxpool12)#CB13\n",
    "Conv16 = Conv2D(128, (3,3), activation='relu',padding='same')(Conv15)\n",
    "Conv16 = BatchNormalization()(Conv16)\n",
    "Conv16 = Dropout(0.5)(Conv16)\n",
    "CMaxpool13 = MaxPooling2D((2,2))(Conv16)\n",
    "\n",
    "\n",
    "Conv21 = Conv2D(32, (5,5), activation='relu')(input_layer)#CB11\n",
    "Conv22 = Conv2D(32, (5,5), activation='relu',padding='same')(Conv21)\n",
    "Conv22 = BatchNormalization()(Conv22)\n",
    "Conv22 = Dropout(0.5)(Conv22)\n",
    "CMaxpool21 = MaxPooling2D((3,3))(Conv22)\n",
    "\n",
    "Conv23 = Conv2D(64, (5,5), activation='relu',padding='same')(CMaxpool21)#CB12\n",
    "Conv24 = Conv2D(64, (5,5), activation='relu',padding='same')(Conv23)\n",
    "Conv24 = BatchNormalization()(Conv24)\n",
    "Conv24 = Dropout(0.5)(Conv24)\n",
    "CMaxpool22 = MaxPooling2D((3,3))(Conv24)\n",
    "\n",
    "Conv25 = Conv2D(128, (5,5), activation='relu',padding='same')(CMaxpool22)#CB13\n",
    "Conv26 = Conv2D(128, (5,5), activation='relu',padding='same')(Conv25)\n",
    "Conv26 = BatchNormalization()(Conv26)\n",
    "Conv26 = Dropout(0.5)(Conv26)\n",
    "CMaxpool23 = MaxPooling2D((2,2))(Conv26)\n",
    "\n",
    "Allnet=concatenate([CMaxpool13,CMaxpool23])\n",
    "\n",
    "AllNet = Flatten()(Allnet)\n",
    "AllNet = Dense(1024, activation='relu')(AllNet)\n",
    "AllNet = Dropout(0.5)(AllNet)\n",
    "\n",
    "predictions = Dense(2, activation=\"softmax\",name='Gender')(AllNet)\n",
    "\n",
    "model = Model(inputs=input_layer, outputs=predictions)\n",
    "model.compile(loss=margin_loss,\n",
    "              optimizer=Adam(lr=0.00001,beta_1=0.9, beta_2=0.999,epsilon=0.00001), metrics = ['accuracy'])\n",
    "\n",
    " \n",
    "model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
    "                              patience=1, min_lr=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history=model.fit(XG,YG1,epochs=1,validation_split=0.1,batch_size=16,shuffle=True,callbacks=[plot,reduce_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.rcParams[\"font.family\"] = \"Times New Roman\"\n",
    "plt.rcParams.update({'font.size': 18})\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.evaluate(XGt, YGt1)\n",
    "print(model.metrics_names, scores)\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1] * 100))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot confusion_matrix\n",
    "pred = model.predict(XGt)\n",
    "pred = np.argmax(pred,axis = 1)\n",
    "y_true = np.argmax(YGt1,axis = 1)\n",
    "CM = confusion_matrix(y_true, pred)\n",
    "fig, ax = mlxtend.plotting.plot_confusion_matrix(conf_mat=CM, figsize=(5, 5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_score = model.predict(XGt)\n",
    "y_test=YGt1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import interp\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import cycle\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "n_classes=2\n",
    "# Plot linewidth.\n",
    "lw = 1.5\n",
    "lw2=1\n",
    "Names=['Fmale', 'Male']\n",
    "# Compute ROC curve and ROC area for each class\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_score[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Compute micro-average ROC curve and ROC area\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test.ravel(), y_score.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "# Compute macro-average ROC curve and ROC area\n",
    "\n",
    "# First aggregate all false positive rates\n",
    "all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "\n",
    "# Then interpolate all ROC curves at this points\n",
    "mean_tpr = np.zeros_like(all_fpr)\n",
    "for i in range(n_classes):\n",
    "    mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "# Finally average it and compute AUC\n",
    "mean_tpr /= n_classes\n",
    "\n",
    "fpr[\"macro\"] = all_fpr\n",
    "tpr[\"macro\"] = mean_tpr\n",
    "roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "colors = cycle(['black', 'red'])\n",
    "for i, color in zip(range(n_classes), colors):\n",
    "    plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n",
    "             label='{0} (area = {1:0.2f})'\n",
    "             ''.format(Names[i], roc_auc[i]))\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=lw2)\n",
    "plt.xlim([-0.02, 1.0])\n",
    "plt.ylim([-0.02, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('')\n",
    "plt.legend(loc='lower center', bbox_to_anchor=(0.5, 1.05), ncol=2, fancybox=False, shadow=False,prop={'size': 13})\n",
    "\n",
    "plt.show()\n",
    "#plt.savefig('F:/Older than 7 jul 20/Chest_G_A_D/AUC_Dis1450NMReal1.png', shadow=True, dpi=1000,transparent=True,bbox_inches=\"tight\",pad_inches=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[20]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(6):\n",
    "    model.layers[i].trainable = False\n",
    "\n",
    "x = model.layers[20].output\n",
    "\n",
    "x = Flatten()(x)\n",
    "x = Dense(1024)(x)\n",
    "x = Activation('relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "output1 = Dense(4, activation = 'softmax',name='Age')(x)\n",
    "model = Model(model.input,output1)\n",
    "\n",
    "# In[11]:\n",
    "\n",
    "\n",
    "model.compile(loss=margin_loss,\n",
    "              optimizer=Adam(lr=0.00001,beta_1=0.9, beta_2=0.999,epsilon=0.00001), metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history=model.fit(XA,YA1,epochs=1,validation_split=0.1,batch_size=16,shuffle=True,callbacks=[plot,reduce_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.rcParams[\"font.family\"] = \"Times New Roman\"\n",
    "plt.rcParams.update({'font.size': 18})\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.evaluate(XAt, YAt1)\n",
    "print(model.metrics_names, scores)\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1] * 100))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot confusion_matrix\n",
    "pred = model.predict(XAt)\n",
    "pred = np.argmax(pred,axis = 1)\n",
    "y_true = np.argmax(YAt1,axis = 1)\n",
    "CM = confusion_matrix(y_true, pred)\n",
    "fig, ax = mlxtend.plotting.plot_confusion_matrix(conf_mat=CM, figsize=(5, 5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_score = model.predict(XAt)\n",
    "y_test=YAt1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import interp\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import cycle\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "n_classes=4\n",
    "# Plot linewidth.\n",
    "lw = 1.5\n",
    "lw2=1\n",
    "Names=['A1', 'A2','A3','A4']\n",
    "# Compute ROC curve and ROC area for each class\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_score[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Compute micro-average ROC curve and ROC area\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test.ravel(), y_score.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "# Compute macro-average ROC curve and ROC area\n",
    "\n",
    "# First aggregate all false positive rates\n",
    "all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "\n",
    "# Then interpolate all ROC curves at this points\n",
    "mean_tpr = np.zeros_like(all_fpr)\n",
    "for i in range(n_classes):\n",
    "    mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "# Finally average it and compute AUC\n",
    "mean_tpr /= n_classes\n",
    "\n",
    "fpr[\"macro\"] = all_fpr\n",
    "tpr[\"macro\"] = mean_tpr\n",
    "roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "colors = cycle(['black', 'red','green','blue'])\n",
    "for i, color in zip(range(n_classes), colors):\n",
    "    plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n",
    "             label='{0} (area = {1:0.2f})'\n",
    "             ''.format(Names[i], roc_auc[i]))\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=lw2)\n",
    "plt.xlim([-0.02, 1.0])\n",
    "plt.ylim([-0.02, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('')\n",
    "plt.legend(loc='lower center', bbox_to_anchor=(0.5, 1.05), ncol=2, fancybox=False, shadow=False,prop={'size': 13})\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(6):\n",
    "    model.layers[i].trainable = False\n",
    "\n",
    "x = model.layers[10].output\n",
    "\n",
    "x = Flatten()(x)\n",
    "x = Dense(1024)(x)\n",
    "x = Activation('relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "output1 = Dense(15, activation = 'softmax',name='dis')(x)\n",
    "\n",
    "model = Model(model.input,output1)\n",
    "\n",
    "# In[11]:\n",
    "\n",
    "\n",
    "model.compile(loss=margin_loss,\n",
    "              optimizer=Adam(lr=0.00001,beta_1=0.9, beta_2=0.999,epsilon=0.00001), metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history=model.fit(X,Y1,epochs=1,validation_split=0.1,batch_size=16,shuffle=True,callbacks=[plot,reduce_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.rcParams[\"font.family\"] = \"Times New Roman\"\n",
    "plt.rcParams.update({'font.size': 18})\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.evaluate(Xt, Yt1)\n",
    "print(model.metrics_names, scores)\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1] * 100))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot confusion_matrix\n",
    "pred = model.predict(Xt)\n",
    "pred = np.argmax(pred,axis = 1)\n",
    "y_true = np.argmax(Yt1,axis = 1)\n",
    "CM = confusion_matrix(y_true, pred)\n",
    "fig, ax = mlxtend.plotting.plot_confusion_matrix(conf_mat=CM, figsize=(15, 15))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_score = model.predict(Xt)\n",
    "y_test=Yt1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import interp\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import cycle\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "n_classes=15\n",
    "# Plot linewidth.\n",
    "lw = 1.5\n",
    "lw2=1\n",
    "Names=['D1_AT', 'D2_CM','D3_CD','D4_ED','D5_EF','D6_EM','D7_FB','D8_HN','D9_IF','D10_MS','D11_ND','D12_NF','D13_PN','D14_PT','D15_PT']\n",
    "# Compute ROC curve and ROC area for each class\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_score[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Compute micro-average ROC curve and ROC area\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test.ravel(), y_score.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "# Compute macro-average ROC curve and ROC area\n",
    "\n",
    "# First aggregate all false positive rates\n",
    "all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "\n",
    "# Then interpolate all ROC curves at this points\n",
    "mean_tpr = np.zeros_like(all_fpr)\n",
    "for i in range(n_classes):\n",
    "    mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "# Finally average it and compute AUC\n",
    "mean_tpr /= n_classes\n",
    "\n",
    "fpr[\"macro\"] = all_fpr\n",
    "tpr[\"macro\"] = mean_tpr\n",
    "roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "colors = cycle(['pink','black', 'red','green', 'orange','blue','purple', 'cyan','brown', 'pink','olive', 'wheat','darkorange','darkblue','lime', 'gray'])\n",
    "for i, color in zip(range(n_classes), colors):\n",
    "    plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n",
    "             label='{0} (area = {1:0.2f})'\n",
    "             ''.format(Names[i], roc_auc[i]))\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=lw2)\n",
    "plt.xlim([-0.02, 1.0])\n",
    "plt.ylim([-0.02, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('')\n",
    "plt.legend(loc='lower center', bbox_to_anchor=(0.5, 1.05), ncol=2, fancybox=False, shadow=False,prop={'size': 13})\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf)",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
